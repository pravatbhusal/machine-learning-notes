{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Regression Models Performance\n",
    "There are multiple methods to evaluate regression model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R Squared\n",
    "<img src=\"images/evaluation/r_squared_example.png\" height=\"75%\" width=\"75%\"></img>\n",
    "\n",
    "- SSres is the sum of residuals using the Least Ordinary Squares method\n",
    "- SStot is the total sum of squares from the average using the Least Ordinary Squares Method\n",
    "- yAvg is the average salary based on the values of all salaries\n",
    "\n",
    "R^2 tells us how \"good\" is our line compared to the average line. It tells us the variance of the observation, in other words how the model is dealing with the \"noise\" (variance) of the data.\n",
    "\n",
    "In order to get the best-fitting line, we run a linear regression to get the \"best\" line compared to the average line. The closer R^2 is to 1, the better the regression line is to model the data set.\n",
    "- Sometimes R^2 can become negative: whenever SSres is a value greater than SStot\n",
    "- Sometimes R^2 can be greater than 1: whenever the data set is too small, has no logical meaning\n",
    "\n",
    "### When To Use R Squared?\n",
    "R Squared is usually best used for linear regression models (simple, multiple, or polynomial). These models typically follow the Least Ordinary Squares method, which utilizes the R^2 methodology to determine a best-fit line.\n",
    "\n",
    "Non-linear models like SVR's RBF kernel should use other error metrics like Root Mean Squared Error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjusted R Squared\n",
    "R Squared encounters a problem if we keep adding new independent variables to a multi-linear model.\n",
    "\n",
    "The problem here is that R^2 will never decrease because adding new variables to the model may not affect the model at all. The fact that we're trying to minimize the sum of squares of the residuals, and if adding a new independent variable only makes R^2 worse, then R^2 will never decrease. This is because the model would just add-in a 0 for the coefficient of the new independent variable to prevent R^2 from decreasing.\n",
    "\n",
    "The R^2 is bias, it will always be increasing and only look for improvements...\n",
    "\n",
    "### Adjusted R Squared Formula\n",
    "Therefore, we need to look for a new method to calculate R^2 called Adjusted R^2.\n",
    "- Unlike R^2, Adjusted R^2 will penalize the model even if new independent variables don't help it\n",
    "\n",
    "<img src=\"images/evaluation/adjusted_r_squared_formula.png\" height=\"50%\" width=\"50%\"></img>\n",
    "- p = the number of independent variables (regressors): the number of columns in the 2D Array\n",
    "- n = number of samples: the number of rows in the 2D Array\n",
    "\n",
    "If we increase the number of independent variables, the ratio ```(n - 1) / (n - p - 1)``` will increase which causes Adjusted R^2 to decrease if the value of R^2 and n are constant.\n",
    "\n",
    "However, even if we increase the number of independent variables but R^2 increases substantially while the value of n is constant, then the value of Adjusted R^2 increases.\n",
    "\n",
    "Therefore, Adjusted R^2 has a balancing mechanism between the value of p and the value of R^2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the linear regression class\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# create relatively linear data\n",
    "x = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([1.2, 2.7, 3.3, 4.5, 5.2])\n",
    "\n",
    "# create a linear regressor Object, then fit it to the training data\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9812014711892113"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the r^2 metric\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# predict the y-values for x\n",
    "y_pred = regressor.predict(x)\n",
    "\n",
    "# compare the actual and predicted values using R^2\n",
    "r2_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating R^2\n",
    "As seen above, the R^2 value is very good because it's a 0.98 (a number very close to 1).\n",
    "\n",
    "Therefore, we can state that the simple linear regression model is highly accurate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
